apiVersion: v1
kind: ConfigMap
metadata:
  name: xtdb-env-config
  namespace: xtdb-deployment
data:
  # Needs to be set based on output of terraform
  XTDB_AZURE_USER_MANAGED_IDENTITY_CLIENT_ID: "aaaaaaaa-aaaa-1111-1111-aaaaaaaaaaaa"
  # Needs to be set based on output of terraform
  XTDB_AZURE_STORAGE_ACCOUNT: "xtdbexamplestorage"
  # Needs to be set based on output of terraform
  XTDB_AZURE_STORAGE_CONTAINER: "xtdbstorage"
  KAFKA_BOOTSTRAP_SERVERS: "kafka-service.xtdb-deployment.svc.cluster.local:9092"
  XTDB_TX_TOPIC: "xtdb-tx-topic"
  XTDB_FILES_TOPIC: "xtdb-files-topic"
  # Configures Java startup options for JVM heap memory and direct memory
  # See: https://docs.oracle.com/cd/E13150_01/jrockit_jvm/jrockit/jrdocs/refman/optionX.html
  JDK_JAVA_OPTIONS: "-Xmx2g -XX:MaxDirectMemorySize=2g -XX:MaxMetaspaceSize=500m"
  # Sets base XTDB logging level
  XTDB_LOGGING_LEVEL: "INFO"
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: xtdb-statefulset
  namespace: xtdb-deployment
  labels:
    app: xtdb-statefulset
spec:
  serviceName: xtdb-service
  replicas: 3
  selector:
    matchLabels:
      app: xtdb-statefulset
  template:
    metadata:
      labels:
        app: xtdb-statefulset
        azure.workload.identity/use: "true" 
    spec:
      # Requires the service account to be created & federated identity set up
      serviceAccountName: xtdb-service-account
      # Depending on node pool being used within AKS
      nodeSelector:
        nodepool: "xtdbpool"
      # Waits for the AKS Kafka to be available before starting XTDB
      initContainers:
      - name: wait-for-kafka
        image: busybox
        command: ['sh', '-c', 'until nc -z kafka-service.xtdb-deployment.svc.cluster.local 9092; do echo waiting for kafka; sleep 5; done;']
        resources:
          requests:
            memory: "256Mi"
          limits:
            memory: "256Mi"
      containers:
      - name: xtdb-container
        # In more production like settings, should be pinned to a specific release
        image: ghcr.io/xtdb/xtdb-azure:nightly
        volumeMounts:
        - name: xtdb-persistent-storage
          mountPath: /var/lib/xtdb/buffers
        # Adjustable, but we would typical recommend XTDB to have 2GiB of Heap Memory, 2GiB of Direct Memory, 500MiB for the metaspace,
        # Alongside extra memory space on the container itself to avoid OoMKilled issues.
        resources:
          requests:
            memory: 6144Mi
          limits:
            memory: 6144Mi
        envFrom:
        - configMapRef:
            name: xtdb-env-config
        env:
        - name: ORDINAL_NUMBER
          valueFrom:
            fieldRef:
              fieldPath: metadata.labels['apps.kubernetes.io/pod-index']
        - name: XTDB_LOCAL_DISK_CACHE
          value: "/var/lib/xtdb/buffers/disk-cache"
        # Identifies node within metrics using a label - using index of stateful set - ie, xtdb-node-0
        # If removed, will have a random suffix that will change on pod restart, which can be seen within logs
        - name: XTDB_NODE_ID
          value: "xtdb-node-$(ORDINAL_NUMBER)"
  volumeClaimTemplates:
  - metadata:
      name: xtdb-persistent-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 50Gi
      storageClassName: managed-csi
---
apiVersion: v1
kind: Service
metadata:
  name: xtdb-service
  namespace: xtdb-deployment
  labels:
    app: xtdb-statefulset
spec:
  type: LoadBalancer
  ports:
    - port: 3000
      name: http
    - port: 5432
      name: pgwire
  selector:
    app: xtdb-statefulset
---
apiVersion: v1
kind: Service
metadata:
  name: xtdb-prometheus
  namespace: xtdb-deployment
  labels:
    app: xtdb-statefulset
spec:
  ports:
    - name: metrics
      port: 80
      protocol: TCP
      targetPort: 8080
  clusterIP: None 
  selector:
    app: xtdb-statefulset
