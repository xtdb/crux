apiVersion: v1
kind: ConfigMap
metadata:
  name: xtdb-env-config
  namespace: xtdb-deployment
data:
  # Needs to be set based on output of terraform
  XTDB_AZURE_USER_MANAGED_IDENTITY_CLIENT_ID: "aaaaaaaa-aaaa-1111-1111-aaaaaaaaaaaa"
  # Needs to be set based on output of terraform
  XTDB_AZURE_STORAGE_ACCOUNT: "xtdbexamplestorage"
  # Needs to be set based on output of terraform
  XTDB_AZURE_STORAGE_CONTAINER: "xtdbstorage"
  KAFKA_BOOTSTRAP_SERVERS: "kafka-service.xtdb-deployment.svc.cluster.local:9092"
  XTDB_TX_TOPIC: "xtdb-tx-topic"
  XTDB_FILES_TOPIC: "xtdb-files-topic"
  # Configures Java startup options for JVM heap memory and direct memory
  # See: https://docs.oracle.com/cd/E13150_01/jrockit_jvm/jrockit/jrdocs/refman/optionX.html
  JDK_JAVA_OPTIONS: "-Xmx2g -XX:MaxDirectMemorySize=2g -XX:MaxMetaspaceSize=500m"
  # Sets base XTDB logging level
  XTDB_LOGGING_LEVEL: "INFO"
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: xtdb-statefulset
  namespace: xtdb-deployment
  labels:
    app: xtdb-statefulset
spec:
  serviceName: xtdb-service
  replicas: 3
  selector:
    matchLabels:
      app: xtdb-statefulset
  template:
    metadata:
      labels:
        app: xtdb-statefulset
        azure.workload.identity/use: "true" 
    spec:
      # Requires the service account to be created & federated identity set up
      serviceAccountName: xtdb-service-account
      # Depending on node pool being used within AKS
      nodeSelector:
        nodepool: "xtdbpool"
      volumes: 
      - name: "tmp"
        emptyDir: {}
      # Waits for the AKS Kafka to be available before starting XTDB
      initContainers:
      - name: wait-for-kafka
        image: busybox:1.37.0
        command: ['sh', '-c', 'until nc -z kafka-service.xtdb-deployment.svc.cluster.local 9092; do echo waiting for kafka; sleep 5; done;']
        resources:
          requests:
            memory: "256Mi"
            cpu: '100m'
          limits:
            memory: "256Mi"
            cpu: '100m'
        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 10001
          runAsGroup: 10001
          readOnlyRootFilesystem: true
          capabilities:
            drop:
              - ALL
          seccompProfile:
            type: RuntimeDefault

      containers:
      - name: xtdb-container
        # In more production like settings, should be pinned to a specific release
        image: ghcr.io/xtdb/xtdb-azure:nightly
        volumeMounts:
        - name: xtdb-persistent-storage
          mountPath: /var/lib/xtdb/buffers
        - name: "tmp"
          mountPath: "/tmp"
        # Adjustable, but we would typical recommend XTDB to have 2GiB of Heap Memory, 2GiB of Direct Memory, 500MiB for the metaspace,
        # Alongside extra memory space on the container itself to avoid OoMKilled issues.
        resources:
          requests:
            memory: 6144Mi
            cpu: '1000m'
          limits:
            memory: 6144Mi
            cpu: '1000m'
        envFrom:
        - configMapRef:
            name: xtdb-env-config
        env:
        - name: ORDINAL_NUMBER
          valueFrom:
            fieldRef:
              fieldPath: metadata.labels['apps.kubernetes.io/pod-index']
        - name: XTDB_LOCAL_DISK_CACHE
          value: "/var/lib/xtdb/buffers/disk-cache"
        # Identifies node within metrics using a label - using index of stateful set - ie, xtdb-node-0
        # If removed, will have a random suffix that will change on pod restart, which can be seen within logs
        - name: XTDB_NODE_ID
          value: "xtdb-node-$(ORDINAL_NUMBER)"

        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
              - ALL
          seccompProfile:
            type: RuntimeDefault
          
        # The following are adjustable based on the exact requirements of the deployment:
        # startupProbe will wait for a while before checking if the pod is considered "started"
        # We check that the node is started & "reasonably" caught up on it's indexes before considering it "started"
        startupProbe:
            httpGet:
              path: /healthz/started
              port: 8080
            initialDelaySeconds: 60
            periodSeconds: 30
            failureThreshold: 10 
        # livenessProbe will check if the node is still alive, and if the indexer hasn't errored out
        # It will run after the first successful startup probe
        livenessProbe:
          httpGet:
            path: /healthz/alive
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          failureThreshold: 3

  volumeClaimTemplates:
  - metadata:
      name: xtdb-persistent-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 50Gi
      storageClassName: managed-csi
---
apiVersion: v1
kind: Service
metadata:
  name: xtdb-service
  namespace: xtdb-deployment
  labels:
    app: xtdb-statefulset
spec:
  type: LoadBalancer
  ports:
    - port: 3000
      name: http
    - port: 5432
      name: pgwire
  selector:
    app: xtdb-statefulset
---
apiVersion: v1
kind: Service
metadata:
  name: xtdb-prometheus
  namespace: xtdb-deployment
  labels:
    app: xtdb-statefulset
spec:
  ports:
    - name: metrics
      port: 80
      protocol: TCP
      targetPort: 8080
  clusterIP: None 
  selector:
    app: xtdb-statefulset
