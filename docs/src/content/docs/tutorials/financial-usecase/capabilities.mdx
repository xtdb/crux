---
title: Time in Finance
---

import Fiddle from '@components/fiddle.astro';
import Txs from '@components/fiddle/txs.astro';
import Query from '@components/fiddle/query.astro';
import QueryTemplate from "@components/fiddle/template.astro"
import DateRange from '@components/fiddle/date-range.astro';

IT systems of all kinds are plagued by many time-related complexities, from misconfigured Network Time Protocol and unexpected certificate expirations, to changing timezones and software concurrency bugs. However, systems in financial domains are additionally accutely affected by the following common requirements and challenges...

## 1) Audit and Replay: what happened?

SQL databases systems typically lose information whenever UPDATE or DELETE are used. They also don't record or track the any changes to data by default.

<Fiddle>
  <Txs txs="INSERT INTO trades (xt$id, price) VALUES (1, 100);" />
  <Txs txs="UPDATE trades SET price = 150 WHERE trades.xt$id = 1;" />
  <Query q={
`SELECT trades.xt$id, trades.price
  FROM trades
`}
/>
</Fiddle>

In common SQL databases, the original price '100' is lost forever, and no information about when it was last changed is recorded. Perhaps if you're sufficiently desparate and very lucky there _might_ be an old backup of the data available somewhere with a previous value.

As a consequence, software engineers working in financial domains are forced to routinely confront this fundamental 'mutability' across their database systems to ensure that sufficient records are kept for satisfying auditors and regulators about exactly how have changed over time.

In XTDB however, there is no need for custom audit tables or timestamp columns to litter the application schema, because a full audit history is maintained automatically, and the history of the entire database can be 'replayed' with fine granularity.

<Fiddle>
  <Txs txs="INSERT INTO trades (xt$id, price) VALUES (1, 100);" />
  <Txs txs="UPDATE trades SET price = 150 WHERE trades.xt$id = 1;" />
  <Query q={
`SELECT trades.xt$id, trades.price, trades.xt$system_from, trades.xt$system_to
  FROM trades FOR ALL SYSTEM_TIME
`}
/>
</Fiddle>

XTDB implements the SQL:2011 standard for "system time versioning" ubiquitously, across all tables. Queries can run against any previous database state and have full access to that history.

## 2) Snapshot-free Reporting: reproducibility without copying and result tearing

Most applications rely on an ability to issue multiple queries against a database, and can't simply execute a single SQL statement to retrieve all necessary information in a single round trip.

To maintain consistency across multiple queries, databases offer ACID transaction sessions which fully isolate the reading application process from any concurrent writes that may be affecting the records being read.

This session-oriented transaction isolation capability is typically stateful and extremely resource-intensive over long durations due to 'locking' of record versions internally within the database, and in turn this degrades the performance of all workloads using the system.

To workaround these limitations of regular transactions, applications that need to run many queries in a consistent manner will often resort to first exporting all the necessary data or using database facilities to create explicit 'snapshots' (i.e. redundant copies).

In contrast, XTDB allows applications to refer to previous database states without any declarations ahead of time - queries can simply specify the version of the database (using a 'basis' timestamp) and specific tables as needed:

<Fiddle>
  <Txs systemTime="2020-01-01"
       txs="INSERT INTO trades (xt$id, price) VALUES (1, 100);" />
  <Txs systemTime="2020-01-02"
       txs="INSERT INTO trades (xt$id, price) VALUES (1, 150);" />
  <Query q={
`SELECT trades.xt$id, trades.price
  FROM trades FOR SYSTEM_TIME AS OF DATE '2020-01-01'
`} />
</Fiddle>

No locking, copying, or snapshotting required. Queries against old, stable versions of the data are always consistent and will never contain partial changes or 'tearing' in the results.

## 3) Interleaving Upstream Sources: who's timestamp?

In many circumstances the system time of the database itself is uninteresting (i.e outside of auditing / debugging), and what is more relevant is the time recorded in an upstrem system. For instance a High-Frequency Trading platform could generate a trade with a timestamp that is anywhere from 0.01-100ms earlier than the time at which the trade database records the trade. In other situations information may even arrive hours or days 'late'.

In cases where the upstream timestamps need to be respected for the AS OF application reporting requirement even moreso than the database timestamps, "valid time versioning" is required (implementing SQL:2011's notion of "application time versioning").

Crucially, valid-time is suitable for handling out-of-order timestamp information coming from many upstream systems, whereas system-time is always a monotonically increasing local clock. All rows of data in XTDB keep track of system-time and valid-time timestamps, which allows for precise querying.

Similarly to using system-time for snapshot-free reporting, valid-time can be used to run multiple queries consistently against fixed timestamps in conjunction with system-time, creating a "bitemporal timeslice" view of data.

<Fiddle autoLoad>
  <Txs systemTime="2020-01-02"
       txs={
`INSERT INTO trades (xt$id, price, xt$valid_from)
VALUES (2, 200, DATE '2020-01-02');`} />
  <Txs systemTime="2020-01-03"
       txs={
`INSERT INTO trades (xt$id, price, xt$valid_from)
VALUES (1, 100, DATE '2020-01-01');`} />
  <Txs systemTime="2020-01-04"
       txs={
`INSERT INTO trades (xt$id, price, xt$valid_from)
VALUES (4, 400, DATE '2020-01-04');`} />

  <QueryTemplate q={
`SELECT trades.xt$id, trades.price
  FROM trades FOR VALID_TIME AS OF {{validTime}}
              FOR SYSTEM_TIME AS OF DATE '2020-01-31'
`} />
  <DateRange name="validTime" dates={["2020-01-01","2020-01-02","2020-01-03","2020-01-04"]} value="2020-01-01" />
</Fiddle>

## 4) Complex Version Histories

In addition to storing and reporting against upstream timestamps, financial applications often need to record and display multiple versions of entities.

Within many SQL systems, schema migrations complicate the ability to retain access to prior versions, and many applications resort to storing data as denormalized JSON.

XTDB allows applications to handle diverse shapes of evolving, document-like data natively within regular SQL. No schema is required up-front and regular SQL typing is respected:

<Fiddle>
  <Txs systemTime="2020-01-02"
       txs={
`INSERT INTO trades (xt$id, info, xt$valid_from)
VALUES (1, {'price': 100}, DATE '2020-01-01');`} />
  <Txs systemTime="2020-01-03"
       txs={
`UPDATE trades
SET info = {'price': 100, 'qty': 5}
WHERE trades.xt$id = 1;`} />
  <Txs systemTime="2020-01-04"
       txs={
`UPDATE trades
SET info = {'price': 101,
            'qty': 4,
	    'model_params':[3.14, DATE '2020-06-01']}
WHERE trades.xt$id = 1;`} />

  <Query q={
`SELECT trades.xt$id, trades.info, trades.xt$valid_from, trades.xt$valid_to
  FROM trades FOR VALID_TIME FROM DATE '2020-01-01' TO DATE '2020-01-31'
`} />
</Fiddle>

## 5) Intelligent Archival: live data and storage tiering with transparent queries

All data has a lifecycle, and in a typical application the window of 'live' data is carefully managed to ensure that the existence of old and now-irrelevant data does not impact the ongoing performance of the application more than necessary.

For instance, once a trade has been settled, it no longer needs to be incorporated into new risk calculations.

However, any such 'old' data invariably still needs to be retained beyond any single application's window of interest, so it likely gets migrated of 'archived' as part of a batch processing job into a longer term storage system which has a significantly lower basic operational cost for data retention.

With XTDB's ubiquitous tracking of `system_to` and `valid_to` columns however, manual migration processes become unnecessary. Applications can allow data to be transparently archived into cheaper storage by the database without affecting the performance of working with live data.

Whenever old data is needed to be queried, it can achieved purely via SQL without any out-of-band engineering or processing work.

Applications can set the `valid_to` column explicitly to correspond with data lifecycle archival events like 'settled_at'. Or alternatively they can simple `DELETE` the relevant records (setting `valid_to` to the implicit `CURRENT_TIMESTAMP` at the time of the transaction):

<Fiddle>
  <Txs systemTime="2020-01-02"
       txs={
`INSERT INTO trades (xt$id, price, xt$valid_from)
VALUES (1, 100, DATE '2020-01-01');`} />
  <Txs systemTime="2020-01-03"
       txs={
`UPDATE trades SET price = 105
WHERE trades.xt$id = 1;`} />
  <Txs systemTime="2020-01-10"
       txs={
`DELETE FROM trades
WHERE trades.xt$id = 1;`} />

  <Query q={
`SELECT trades.xt$id, trades.price, trades.xt$valid_from, trades.xt$valid_to
  FROM trades FOR VALID_TIME FROM DATE '2020-01-01' TO DATE '2020-01-31'
`} />
</Fiddle>

Only systems with a comprehensive, fine-grained understanding of the data lifecycle can effectively implement storage tiering to reduce long-term costs.

## 6) Corrections: curated history

Many finance applications rely on carefully designed 'forward correction' mechanisms.

## 7) Scheduled Effectivity: synchronize and preview future states

## 8) What If: time-travelling source of truth